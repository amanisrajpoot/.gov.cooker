# .gov.cooker Content Policy & Moderation Guidelines

## Mission Statement

Our platform exists to make corruption and non-delivery visible, verifiable, and costly for officials while protecting citizens and whistleblowers. This content policy ensures our platform remains a safe, constructive space for civic engagement.

## Core Principles

1. **Transparency**: All moderation decisions are logged and auditable
2. **Safety**: Protect users from harm while preserving free speech
3. **Accountability**: Hold officials accountable through verified evidence
4. **Privacy**: Respect user privacy and consent

## Content Categories

### ✅ Allowed Content

- **Verified Reports**: Factual reports with evidence about government issues
- **Petition Campaigns**: Legitimate campaigns targeting specific officials or policies
- **RTI Requests**: Right to Information requests and responses
- **Public Data**: Government datasets, budgets, tenders, and contracts
- **Constructive Discussion**: Civil discourse about public issues
- **Evidence**: Photos, videos, documents supporting claims (with metadata stripped)

### ⚠️ Restricted Content

- **Unverified Claims**: Allegations without supporting evidence
- **Personal Attacks**: Ad hominem attacks on individuals
- **Sensitive Information**: Personal details of private citizens
- **Commercial Content**: Spam, advertising, or promotional material
- **Off-topic Content**: Content unrelated to government accountability

### ❌ Prohibited Content

- **Illegal Content**: Content that violates applicable laws
- **Violence**: Threats, incitement to violence, or graphic violence
- **Harassment**: Targeted harassment, doxxing, or intimidation
- **Hate Speech**: Content targeting groups based on protected characteristics
- **Disinformation**: Deliberately false information intended to mislead
- **CSAM**: Child sexual abuse material (automatic detection and reporting)
- **Terrorism**: Content promoting or supporting terrorist activities
- **Fraud**: Scams, phishing, or fraudulent schemes

## Moderation Process

### 1. Automated Screening

- **AI Detection**: Machine learning models flag potentially problematic content
- **Metadata Analysis**: Automatic stripping of identifying metadata from uploads
- **Duplicate Detection**: Perceptual hashing to identify duplicate content
- **Spam Filters**: Rate limiting and pattern detection for spam

### 2. Human Review

- **Moderator Queue**: Flagged content reviewed by trained moderators
- **Escalation**: Complex cases escalated to senior moderators
- **Expert Review**: Legal and technical experts for specialized content
- **Community Reports**: User-reported content prioritized for review

### 3. Decision Framework

#### Immediate Removal
- Illegal content
- CSAM
- Threats of violence
- Doxxing
- Spam

#### Temporary Restriction
- Unverified claims pending evidence
- Content under investigation
- Potential privacy violations

#### Warning/Education
- First-time violations of minor policies
- Educational content about platform guidelines
- Suggestions for improvement

#### No Action
- Content that complies with policies
- False positives from automated systems
- Legitimate civic engagement

## Appeal Process

### 1. Internal Appeal
- Users can appeal moderation decisions within 7 days
- Appeals reviewed by different moderator than original decision
- Response within 48 hours

### 2. Independent Review
- Complex cases reviewed by independent panel
- Panel includes legal experts, community representatives
- Final decision within 14 days

### 3. Transparency Report
- Quarterly reports on moderation actions
- Aggregate statistics (no personal information)
- Public disclosure of policy changes

## User Safety Measures

### 1. Anonymity Options
- Anonymous reporting for sensitive issues
- Pseudonymous participation
- Metadata stripping on all uploads

### 2. Whistleblower Protection
- Client-side encryption for sensitive reports
- Secure communication channels
- Legal support for legitimate whistleblowers

### 3. Harassment Prevention
- Block and report functionality
- Rate limiting on user actions
- Shadow banning for repeat offenders

## Evidence Standards

### 1. Acceptable Evidence
- **Photos**: Clear, unedited images with timestamps
- **Videos**: Unedited footage with metadata
- **Documents**: Official documents, contracts, correspondence
- **Data**: Government datasets, financial records

### 2. Evidence Requirements
- **Authenticity**: Must be verifiable and unaltered
- **Relevance**: Directly related to the reported issue
- **Privacy**: No personal information of private citizens
- **Consent**: Proper consent for any personal data

### 3. Verification Process
- **Technical Analysis**: Metadata analysis, reverse image search
- **Cross-referencing**: Comparison with other sources
- **Expert Review**: Technical experts for specialized content
- **Community Verification**: Crowdsourced fact-checking

## Transparency and Accountability

### 1. Moderation Logs
- All moderation actions logged with timestamps
- Audit trail for all decisions
- Regular review of moderation patterns

### 2. Public Reporting
- Quarterly transparency reports
- Statistics on moderation actions
- Policy updates and changes

### 3. User Rights
- Right to appeal moderation decisions
- Right to data portability
- Right to deletion (with legal exceptions)

## Legal Compliance

### 1. Jurisdictional Requirements
- Compliance with local laws in each jurisdiction
- Data protection regulations (GDPR, etc.)
- Content removal requests from authorities

### 2. Law Enforcement
- Cooperation with legitimate law enforcement requests
- Preservation of evidence for legal proceedings
- Protection of user privacy within legal bounds

### 3. International Standards
- Human rights principles
- Freedom of expression
- Right to information

## Training and Education

### 1. Moderator Training
- Regular training on policy updates
- Cultural sensitivity training
- Legal compliance training
- Technical training on content analysis

### 2. User Education
- Clear guidelines and examples
- Educational content about civic engagement
- Best practices for evidence collection
- Safety tips for whistleblowers

### 3. Community Guidelines
- Regular updates to community guidelines
- User feedback on policy changes
- Community input on moderation practices

## Enforcement Actions

### 1. Warning System
- **First Offense**: Warning and education
- **Second Offense**: Temporary restriction (24-48 hours)
- **Third Offense**: Longer restriction (1-7 days)
- **Repeated Offenses**: Account suspension or ban

### 2. Account Actions
- **Temporary Suspension**: 1-30 days for policy violations
- **Permanent Ban**: For severe or repeated violations
- **Shadow Ban**: For spam or harassment (content hidden from others)

### 3. Content Actions
- **Removal**: Immediate removal of prohibited content
- **Restriction**: Limited visibility for borderline content
- **Flagging**: Marking content for review
- **Editing**: Minor edits to remove sensitive information

## Contact and Support

### 1. Moderation Team
- Email: moderation@gov-cooker.org
- Response time: 24-48 hours
- Escalation: security@gov-cooker.org

### 2. Legal Team
- Email: legal@gov-cooker.org
- For legal requests and compliance issues
- Response time: 48-72 hours

### 3. Technical Support
- Email: support@gov-cooker.org
- For technical issues and bugs
- Response time: 24-48 hours

## Policy Updates

This content policy is reviewed quarterly and updated as needed. Users will be notified of significant changes 30 days in advance.

**Last Updated**: [Date]
**Next Review**: [Date]
**Version**: 1.0

---

*This policy is designed to balance free expression with safety and accountability. Our goal is to create a platform where citizens can effectively hold their government accountable while maintaining a respectful, constructive community.*
